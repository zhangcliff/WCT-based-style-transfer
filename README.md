# WCT-based-style-transfer
This is a  tensorflow implementation of WCT based style transfer

WCT based style transfer was proposed by [Universal Style Transfer via Feature Transforms.](http://xueshu.baidu.com/s?wd=paperuri%3A%28af912f3490e8e1a6c23a027c8aa87cd8%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fabs%2F1705.08086&ie=utf-8&sc_us=12956352176356800874)
It's core idea is to train a decoder to reconstruct general image from features which are generated by a pretrained VGG19.
When testing, we use a WCT layer to blend the features of content and style image. Then the trained decoder decodes the blended features to generate stylized image.
In the original paper, they trained 5 decoders for layer reluX(x=1,2,3,4,5) separately, and use them all to carry out multi-level stylization.

![image](https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/result/network.png)

In this repository, we only trained 4 decoders for layer relu1-4.

## Samples
<div align=center><img width="244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/content/im4.jpg" >
<img width = "244" height="244" src ="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/style/s5.jpg">
<img width = "244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/result/result_1.jpg">
<br>
<img width="244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/content/im2.jpg" >
<img width = "244" height="244" src ="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/style/s2.jpg">
<img width = "244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/result/result_3.jpg">
<br>
<img width="244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/content/im3.jpg" >
<img width = "244" height="244" src ="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/style/s3.jpg">
<img width = "244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/result/result_4.jpg">
<br>
<img width="244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/content/im1.jpg" >
<img width = "244" height="244" src ="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/style/s1.jpg">
<img width = "244" height="244" src="https://github.com/zhangcliff/WCT-based-style-transfer/blob/master/result/result_2.jpg">
 <br> 
 <div align=left>  
  
## Test
1 download the pre-trained vgg19 weights [VGG19](https://pan.baidu.com/s/1zpsUu-V9taVBoaqBLK_OuQ)
 <br>
2 download the 4 trained decoders [weights](https://pan.baidu.com/s/1wencvm0bESOU_s5k2wRmYQ)
```shell
mkdir models
```
and put the trained decoders weights to models folder
 <br>
Then run :
```shell
python test_all_layer.py --content_path /path/to/content_img --style_path /path/to/style_img  --output_path /path/to/result_img --pretrained_vgg path/to/vgg19 --alpha 1
 ```
## Train
1 download the content images from [coco](http://msvocds.blob.core.windows.net/coco2014/train2014.zip)
<br>
2 make tfrecord file 
```shell
mkdir tfrecords
python gen_tfrecords.py
```
<br>
3 we need to train decoders for layer relu1-4 seprartely. If we want to train decoder for relu4 , wo can run as :
```shell
python train.py --target_layer relu4 --pretrained_path path/to/vgg19 --max_iterator 20000 --checkpoint_path path/to/save_checkpoint --tfrecord_path path/to/tfrecord  --batch_size 8
```
